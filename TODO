X Make cross validation datasets
X Make training routine that recognises kfold
X Put make the evaluation script recognise kfold. Evaluation get saved to a json file, so I just need to join the different folds.
X Make dataset_stats.py recognise kfold
X The output folder from the best config seems to get deleted by the kfold testing.
X K1 seems to find its images and labels fine, subsequent k seems to not find labels.
X Test the pipeline - up to offline_validation
X Make format_confusion_matrix get the right results.
X Fold 3 seems to have zero positive examples - is this true - can it be avoided?

Does the current background inclusion method work well?
    It does not make a very clear difference as far as I can see.
    It might reduce the number of needed epochs, but this is probably because the signal data is seen multiple times within the same epoch through resampling.
    It might make more sense to look at each signal example once, alongside a random selection of background, rather than the other way around. This would be much faster.

Correct the loss function weighting.
    The Focal Loss should actually not be taking weights, so I don't think it's correct. Can just use Lovasz loss and gloss over it.

Where does the "land area / built area" table come from? Is it in the greenroofs_analysis folder?

Look at the previous results of hyperparam scanning. Can we trust them? Probably not.
    Taking a look with a reduced dataset they all seem to top out relatively low. Nothing does better than about a f=0.7.
    On the one hand, this means that it doesn't matter that much what hyperparams we choose.
    On the other, it's still not great performance.
    The test performance of the dummy run with only 2 epochs seems to do better than my "well trained" model did! Is this because of the f1 calculation error? It could also be because of the smaller number of background tiles included in validation. Or, that is simply the maximum achievable - in which case the training data needs improvement? Do the augmentations really make a difference?
        Alternative augs might be spatially jittering - account for imperfect labelling? Maybe the augs would make a difference to the alternative test datasets. 
        Implementing more augs is tempting but likely to lead to delays.
    It would make sense to test freezing the pretrain again.
    Given performance is not that good, I feel like a really long train with a low learning rate could help? Then again, there is no reason for this to help actually.
        There is no longer as large a gap in performance between training and validation, probably because of the bug. Therefore, it might stand up to more epochs.

Make alternative test datasets using alternative data
Test the background resampling method on the laptop.
Consider a hard example mining method instead.
Update the CFF
Use rasterio? instead of their extraction routine in extract.py. Use buffer operations in shapely instead of filters.


We could justify running predictions for later imagery if it performs well.

I suspect there could still be a problem with imbalance, given that the focal loss might not have been properly implemented.
It would be good to see if the augmentations improve performance on imagery from a different set.

Priorities:
- Is performance similar for best config and frozen pretrained?
    Yes
- Investigate imbalance - does weighting or focal improve things?
    Focal gives similar results
    Weights seems to give more slow-and-steady training with the same LR. Resulted in early stopping with worse performance.
    What about weights + focal?
    I think I had the weights back-to-front
    I think it affects the balance of precision and recall
- Does randomperspective help? Without adjusting the labels?
    The augs don't seem to help much in general
- Alternative test datasets
- Merge changes from myriad.
- Can I look at what areas are misclassified?
- Update the CFF
- Rasterio
- The more things you leave alone the sooner you will be done.

It looks like I already made an "alternative" evaluation dataset - what did I do? It's from mapbox.

-----------------------
Deadline?
x Download alternative dataset -> newest getmapping imagery
x Check the labelling accuracy for the alternative imagery sets - this is difficult and could be a lot of work.  - Check for labelling mistakes in the already generated predictions
x Transfer raw imagery to myriad - will need to clear space from ERA5 data
- Test effect of buffering the training polygons by 0.5 m 
x Tiling - transfer the raw data to myriad and batch the tiling by 10km tile (use the 10km tile containing the centroid of the smaller tile so there are no overlaps):
    - newest getmapping
    - older getmapping
    - geomni london - isn't ortho! 
    - what about planet? what about mapbox?
x Merge changes to/from myriad
x Check the weighting matches the weighting of the validation dataset.
- Should I be using background resampling? It does seem pretty effective for suppressing background - use the original settings.
x Integrate changes to polygonization - this requires removing the dependence on mercantile and SlippyMap assumptions.
x Test methods on laptop

- Integrate alternative validation sets into validation - by editing dataset.py and adding an extra loader in experiment.py

- Run training - on the laptop? or on myriad as will have batched the imagery splitting there.
- Run prediction - on myriad
- Generate results for paper and update repo
- Changes to the manuscript


